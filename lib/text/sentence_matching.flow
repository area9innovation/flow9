import ds/trie;
import ds/set;
import text/dictionary_levenshtein;
import math/md5;
import ds/limitedheap;

export {
	// Given a set of sentences, we prepare a matching device that can help us do
	// auto-completion to these
	buildSentenceMatcher(sentences : [string]) -> SentenceMatcher;

	// buildSentenceMatcher uses this function to split sentences to words
	splitSentenceToWords(sentence : string) -> [string];

	// Will find all sentences that match the given input. If prefix is true, we will also suggest
	// completions of the final word only. If exhaustive is false, it will give an approximation
	// to the correct result, but relatively fast. If true, it will give exact results slowly.
	// It doesn't consider position : int argument, but it's needed for global consistency
	approximateSentenceMatch(matcher : SentenceMatcher, input : string,
		maxHits : int, prefix : bool, exhaustive : bool, position : int) -> [SentenceMatch];

	// The simplest sentence matcher with a simple check whether sentence from the list contains input or not
	simpleSentenceMatcher(matcher : SentenceMatcher, input : string,
		maxHits : int, prefix : bool, exhaustive : bool, position : int) -> [SentenceMatch];

	// It's looking for a [SentenceMatch] for the particular current word depending on
	// cursor position at input string.
	anyPositionWordMatcher(matcher : SentenceMatcher, input : string,
		maxHits : int, prefix : bool, exhaustive : bool, position : int) -> [SentenceMatch];

	// This is a function for TMatchedHandler style.
	// It allows to replace only particular current word by matched word instead of
	// replacing the whole input string as it's happening by default.
	anyPositionMatchedWordHandler(_matched : string, input : string, position : int) -> string;

	SentenceMatch ::= SentenceHit, SentenceHitExtended;
		SentenceHit(matcher : SentenceMatcher, sentence : string, score : double, origWords : Set<string>);
		SentenceHitExtended(matcher : SentenceMatcher, sentence : string, score : double, origWords : Set<string>, index : int);

	// We build a Trie of the words, give each word an id, and represent the sentence
	SentenceMatcher(
		// The original array of sentences
		sentences : [string],
		// A trie of the words for dictionary levenshtein matching
		words : Trie<SentenceWord>,
		// Each word gets an id
		wordIds : Tree<int, string>,
		// Each sentence is considered a bag of words.
		// The sentences are numbered from the original array
		sentenceWordBags : Tree<int, Set<int>>,
		// Lookup from a word id to which sentences include this word
		word2sentence : Tree<int, Set<int>>
	);

	SentenceWord(word : string, frequency : int, wordid : int);

	dummySentenceMatcher = SentenceMatcher([], makeTrie(), makeTree(), makeTree(), makeTree());
	simpleBuildSentenceMatcher = \sentences -> SentenceMatcher(sentences, makeTrie(), makeTree(), makeTree(), makeTree());

	getCurrentWordPosition(input : string, position : int, sentences : [string]) -> Pair<int, int>;
}





// Temporary structure used to build our matching data structure
SentenceMatcherAcc(
	mutable word2id : Tree<string, int>,
	mutable wordIds : Tree<int, string>,
	sentenceWordBags : Tree<int, Set<int>>,
	mutable word2sentence : Tree<int, Set<int>>,
	mutable nextWordId : int,
);

sentenceMatcherCache : ref Tree<string, SentenceMatcher> = ref makeTree();

buildSentenceMatcher(_sentences : [string]) -> SentenceMatcher {
	makeMatcher = \sentences -> {
		macc = foldi(
				sentences,
				SentenceMatcherAcc(makeTree(), makeTree(), makeTree(), makeTree(), 0),
				\sentenceId, acc1 : SentenceMatcherAcc, sentence -> {
			//all sentences toLowerCase
			words = toLowerCase(sentence) |> splitSentenceToWords;

			wordbag = fold(words, makeSet(), \acc2 : Set<int>, word -> {
				// Find a word id for this word
				wordId1 = lookupTreeDef(acc1.word2id, word, -1);
				wordId2 = if (wordId1 == -1) {
					// New word, find an id and record it
					id = acc1.nextWordId;
					acc1.nextWordId ::= acc1.nextWordId + 1;
					acc1.word2id ::= setTree(acc1.word2id, word, id);
					acc1.wordIds ::= setTree(acc1.wordIds, id, word);
					id;
				} else {
					wordId1;
				}

				// Record this sentences has this word
				insertSet(acc2, wordId2);
			});

			foldSet(wordbag, 0, \dummy, wordId -> {
				sents = lookupTreeDef(acc1.word2sentence, wordId, makeSet());
				acc1.word2sentence ::= setTree(acc1.word2sentence, wordId, insertSet(sents, sentenceId));
				dummy
			});

			newWordBags = setTree(acc1.sentenceWordBags, sentenceId, wordbag);

			SentenceMatcherAcc(acc1.word2id, acc1.wordIds, newWordBags, acc1.word2sentence, acc1.nextWordId);
		});

		trie = foldTree(macc.word2id, makeTrie(), \word, id, acc0 -> {
			// Use the number of sentences this word appears in as frequency here
			inSentences = lookupTreeDef(macc.word2sentence, id, makeSet());
			freq = sizeSet(inSentences);
			addTrie1(acc0, word, SentenceWord(word, freq, id));
		});

		SentenceMatcher(sentences, trie, macc.wordIds, macc.sentenceWordBags, macc.word2sentence);
	}

	hash = md5(strGlue(_sentences, ""));
	switch(lookupTree(^sentenceMatcherCache, hash)){
		None(): {
			new = makeMatcher(_sentences);
			sentenceMatcherCache := setTree(^sentenceMatcherCache, hash, new);
			new
		}
		Some(m): m;
	}

}

splitSentenceToWords(sentence : string) -> [string] {
	separators = ["-", "/", ".", "_"];
	replaces = fold(separators, [], \acc, s -> concat(acc, [s, " "]));
	dehype = strReplaces(sentence, replaces);
	filter(map(strSplit(dehype, " "), \w -> trim2(w, "(),.;:!-'\"")), neq(""));
}

SentenceWordHit(originalWord : string, word : string, frequency : int, wordId : int, distance : double, sentences : Set<int>);

approximateSentenceMatch(matcher : SentenceMatcher, input : string, maxHits : int, prefix : bool, exhaustive : bool, __ : int) -> [SentenceMatch] {
	words = splitSentenceToWords(input);
	n = length(words);
	trailingSpace = endsWith(input, " ");
	matches : [[SentenceWordHit]] = mapi(words, \i, word -> {
		isLastIncompleteWord = prefix && i == n - 1 && !trailingSpace; // For the last word, without trailing space, when requested, we do prefixes
		l = strlen(word);
		dictHits = approximateDictionarySearch(matcher.words, word, 2, isLastIncompleteWord, maxHits);
		wordHits = filtermap(dictHits, \d : DictionaryHit<SentenceWord> -> {

			sl = if (isLastIncompleteWord) strlen(d.word) else l;

			score = relativeLevenshteinDistance(d.distance, max(sl, l));
			if (score > 0.5) {
				wordId = d.cookie.wordid;
				sentences = lookupTreeDef(matcher.word2sentence, wordId, makeSet());
				sh = SentenceWordHit(word, d.word, d.cookie.frequency, wordId, score, sentences);
				Some(sh)
			} else {
				None()
			}
		});

		mergeSort(wordHits, \h1 : SentenceWordHit, h2 : SentenceWordHit -> {
			genericCompareDesc(h1.distance, h2.distance);
		})
	});

	if (false) {
		iter(matches, \ms -> {
			println(foldi(ms, "", \i : int, s, m -> {
				s + (if (i == 0) m.originalWord + ": " else "") + " " + m.word + "(" + d2s(m.distance) + ")";
			}));
		});
	}

	// First, we look at the sentences of the top scorer for each word, and see if there are sentences
	// that match all of these words. That would be perfect
	topsentences = fold(matches, makeSet(), \acc, ms : [SentenceWordHit] -> {
		if (ms != []) {
			if (isEmptySet(acc)) ms[0].sentences
			else intersectSets(acc, ms[0].sentences)
		} else acc
	});

	// If we have a non-empty set, then we use that as the best candidates to check first
	it = if (isEmptySet(topsentences)) makeTree() else {
		tv = fold(matches, SentenceWordHit("", "", 0, 0, 0.0, topsentences), \acc, ms -> if (ms != []) ms[0] else acc);
		topwordhit = SentenceWordHit(tv.originalWord, tv.word, tv.frequency, tv.wordId, tv.distance, topsentences);
		makeTree1(0, [topwordhit]);
	}

	// For the rest of the potential candidate sentences, we look for the words that give
	// the smallest sentence sets first, since those are most discriminative
	cands : Tree<int, [SentenceWordHit]> = fold(matches, it, \acc, ms : [SentenceWordHit] -> {
		// We only take the 5 best word matches, to avoid polluting with garbage
		fold(subrange(ms, 0, 4), acc, \acc2, m : SentenceWordHit -> {
			k = sizeSet(m.sentences);
			// combscore = log(i2d(k + 10)) / m.distance;
			treePushToArrayValue(acc2, k, m);
		});
	});

	// With everything ready, let us go and inspect a bunch of sentences in detail
	checkTopCandidates(matcher, matches, [], cands, makeSet(), if (exhaustive) 1000000 else 50, maxHits);
}

checkTopCandidates(matcher : SentenceMatcher, matches : [[SentenceWordHit]], bestHits : [SentenceHit],
		cands : Tree<int, [SentenceWordHit]>, checked : Set<int>, slots : int, maxHits : int) -> [SentenceMatch] {
	minimum = popmin(cands);
	switch (minimum) {
		EmptyPopResult(): bestHits;
		PopResult(mk, mv, mrest): {
			hits = minimum.v;
			checked1 : Pair<Set<int>, [SentenceHit]> = fold(hits, Pair(checked, bestHits), \acc : Pair<Set<int>, [SentenceHit]>, h : SentenceWordHit -> {
				// TODO: Instead of ignoring the set completely when it is too big,
				// we should just take "slots" items and look at
				if (sizeSet(acc.first) < slots) {
					foldSet(h.sentences, acc, \acc2 : Pair<Set<int>, [SentenceHit]>, sentenceId -> {
						if (containsSet(acc2.first, sentenceId)) {
							// Already checked
							acc2;
						} else {
							// Checking sentence
							score = scoreSentence(matcher, matches, sentenceId);

							if (score.first < 0.0) {
								acc2;
							} else {
								hit = SentenceHit(matcher, matcher.sentences[sentenceId], score.first, score.second);

								// Add the result to the list of hits in sorted order
								newHits = arrayPushSortedWithLimit(acc2.second, hit, \sh1 : SentenceHit, sh2  : SentenceHit -> {
									genericCompareDesc(sh1.score, sh2.score)
								}, maxHits);

								Pair(insertSet(acc2.first, sentenceId), newHits);
							}
						}
					});
				} else acc;
			});
			if (sizeSet(checked1.first) < slots) {
				checkTopCandidates(matcher, matches, checked1.second, minimum.rest, checked1.first, slots, maxHits);
			} else checked1.second;
		}
	}
}

scoreSentence(matcher : SentenceMatcher, matches : [[SentenceWordHit]], sentenceId : int) -> Pair<double, Set<string>> {
	sentenceWords = lookupTreeDef(matcher.sentenceWordBags, sentenceId, makeSet());
	// println("Checking " + matcher.sentences[sentenceId]);

	hits = fold(matches, Triple(0.0, sentenceWords, makeSet()), \acc : Triple<double, Set<int>, Set<string>>, ms : [SentenceWordHit] -> {
		// Find the best word
		wordScore : Pair<double, int> = foldi(ms, Pair(-1.0, -1), \i : int, acc2 : Pair<double, int>, m -> {
			if (acc2.first == -1.0) {
				if (containsSet(acc.second, m.wordId)) {
					// The worse the hit, the less we grant it
					hitscore = max(1.0 - i2d(i) * 0.1, 0.0);
					Pair(hitscore * m.distance, m.wordId);
				} else acc2;
			} else acc2;
		});
		if (wordScore.first == -1.0) {
			// Superfluos word
			if (ms == []) {
				// Not supposed to happen, but what the hell
				acc;
			} else {
				// Give some penalty
				representative = ms[0];
				origWord = representative.originalWord;
				// The penality is based on the length of the word
				lengthPenalty = i2d(min(strlen(origWord), 10));

				// As well as how many sentences it is used in at the most
				freq = sizeSet(representative.sentences);
				freqPenalty = dround(if (freq == 0) 0.0 else 40.0 / log(i2d(freq + 1)));

				penality : double = lengthPenalty + freqPenalty;

				// println("Superfluous " + origWord + " interpreted as " + representative.word + " gives penalty " + d2s(lengthPenalty) + " plus " + d2s(freqPenalty) + " for frequency");
				Triple(acc.first - penality, acc.second, acc.third)
			}
		} else {
			// Word match
			// println("Hit on " + i2s(wordScore.second));

			representative = ms[0];
			origWord = representative.originalWord;

			wordId = wordScore.second;
			word = lookupTreeDef(matcher.wordIds, wordId, "");

			lengthBonus = i2d(min(strlen(origWord), 10));

			sentenceSet = lookupTreeDef(matcher.word2sentence, wordId, makeSet());
			freq = sizeSet(sentenceSet);
			frequencyScore = dround(if (freq == 0) 0.0 else 40.0 / log(i2d(freq + 1)));

			matchScore = wordScore.first * wordScore.first;

			score = matchScore * (10.0 * lengthBonus + frequencyScore);

			// println("Hit on " + word + " gives " + d2s(matchScore) + " with length "+ d2s(10.0 * lengthBonus) + " plus " + d2s(frequencyScore) + " for frequency to total " + d2s(score));

			Triple(acc.first + score, removeSet(acc.second, wordId), insertSet(acc.third, origWord));
		}
	});

	// Missing words
	result = foldSet(hits.second, hits.first, \acc : double, wordId : int -> {
		word = lookupTreeDef(matcher.wordIds, wordId, "");

		// The penality is based on the length of the word
		lengthPenalty = i2d(min(strlen(word), 10));

		// As well as how many sentences it is used in
		sentenceSet = lookupTreeDef(matcher.word2sentence, wordId, makeSet());
		freq = sizeSet(sentenceSet);
		freqPenalty = if (freq == 0) 0.0 else 10.0 / log(i2d(freq + 1));

		penality : double = 0.3 * (lengthPenalty + freqPenalty);

		// println("Missing " + word + " gives penalty " + d2s(lengthPenalty) + " plus " + d2s(freqPenalty) + " for frequency for total " + d2s(penality));
		acc - penality
	});

	// println("Gives " + d2s(result) + "\n");
	Pair(result, hits.third);
}

anyPositionWordMatcher(matcher : SentenceMatcher, input : string, maxHits : int, prefix : bool, exhaustive : bool, position : int) -> [SentenceMatch] {
	if (input == "") []
	else {
		cw = getCurrentWordPosition(input, position, matcher.sentences);

		currentWord = toLowerCase(strsubsmart(input, cw.first, cw.second));

		if (currentWord == "") []
		else {
			fold(matcher.sentences, [], \acc, sen ->
				if (strContains(toLowerCase(sen), currentWord))
					arrayPush(acc, SentenceHit(matcher, sen, 10.0, makeSet()))
				else acc
			)
		}
	}
}

simpleSentenceMatcher(matcher : SentenceMatcher, input : string, maxHits : int, prefix : bool, exhaustive : bool, position : int) -> [SentenceMatch] {
	lowerInput = toLowerCase(input);

	if (input == "")
		[]
	else {
		matchesHeap = foldi(matcher.sentences, makeLimitedHeap(maxHits, makeTree()), \i, heap, sen -> {
			pos = strIndexOf(toLowerCase(sen), lowerInput);

			if (pos >= 0) {
				score = min(intMax, max(0, intMax / (pos + 1) - strlen(sen) + strlen(input))) |> i2d;
				insertLimitedHeap(heap, score, Pair(sen, i));
			} else heap
		});

		map(limitedHeap2array(matchesHeap), 
				\scoreAndSentence -> {
					SentenceHitExtended(matcher, scoreAndSentence.second.first, scoreAndSentence.first, makeSet(), scoreAndSentence.second.second);
				}
		);
	}
}

anyPositionMatchedWordHandler(_matched : string, input : string, position : int) -> string {
	if (input == "") _matched
	else {
		cw = getCurrentWordPosition(input, position, [_matched]);

		strRemove(input, cw.first, cw.second)
			|> \s -> strInsert(s, _matched, cw.first)
	}
}

getCurrentWordPosition(input : string, position : int, sentences : [string]) -> Pair<int, int> {
	//if matcher sentences contain a character, it shouldn't be a separator
	separators = filter(["-", "+", "/", "*", ".", "=", "(", ")"],
		\separator -> !exists(sentences, \sentence -> strContains(sentence, separator)));
	space = " ";
	// replace all separators by space " "
	input1 = interleave(separators, space)
		|> (\arr -> arrayPush(arr, space))
		|> (\arr -> strReplaces(input, arr));

	rightPart = strRight(input1, position);
	leftSpaceIndex = strLastIndexOf(strLeft(input1, position), space);
	rightSpaceIndex = strIndexOf(rightPart, space);

	startCurrentWord = if (leftSpaceIndex == -1) 0 else leftSpaceIndex + 1;
	lengthCurrentWord = (if (rightSpaceIndex == -1) strlen(rightPart) else rightSpaceIndex) + position - startCurrentWord;

	Pair(startCurrentWord, lengthCurrentWord)
}
/*
main() {
	sentences = [
		"The quick brown fox jumps over the lazy dog",
		"The man and the woman walks a lot over brown dirt",
		"Everything has its beauty but not everyone sees it",
		"Ignorance is the night of the mind, but a night without moon and star",
		"It does not matter how slowly you go so long as you do not stop",
		"Our greatest glory is not in never falling, but in getting up every time we do",
		"What the superior man seeks is in himself; what the small man seeks is in others",
		"They must often change who would be constant in happiness or wisdom",
		"I am not one who was born in the possession of knowledge; I am one who is fond of antiquity, and earnest in seeking it there",
		"If a man takes no thought about what is distant, he will find sorrow near at hand",
		"Learning without thought is labor lost; thought without learning is perilous",
		"Recompense injury with justice, and recompense kindness with kindness",
		"Kindness",
		"Everything",
	];
	grapes = strSplit("#include sandbox/tropic/grapes.txt", "\n");

	matcher = buildSentenceMatcher(grapes);

	hits = approximateSentenceMatch(matcher, "pi", 10, true, -1);
	println(strGlue(map(hits, \h -> toString(h)), "\n"));

	quit(0);
}
*/
